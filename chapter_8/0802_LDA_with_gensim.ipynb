{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# want to make clean words and return a list of tokens\n",
    "\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCREEN_NAME', 'said', 'the', '#', 'chicken', 'was', 'at', 'the', '#', 'junkyard', '.', 'see', 'URL', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = '@bob said the #chicken was at the #junkyard. See http://www.jonathanmugan.com.'\n",
    "out_tokens = tokenize(sent)\n",
    "print(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want to lemmatize so dogs goes to dog and ran goes to run\n",
    "# Lemmatizations means to get the \"dictionary entry\" for a word\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "# or can use this\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs dog dog\n",
      "ran run ran\n",
      "discouraged discourage discouraged\n"
     ]
    }
   ],
   "source": [
    "for w in ['dogs', 'ran', 'discouraged']:\n",
    "    print(w, get_lemma(w), get_lemma2(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'going', 'restaurant', 'hamburger']\n"
     ]
    }
   ],
   "source": [
    "sent = 'I enjoy going to restaurants to eat hamburgers.'\n",
    "print(prepare_text_for_lda(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spend', 'watching', 'finding', 'bigfoot', 'marathon', 'animal', 'planet', 'fear', 'death', 'sasquatch', 'summer']\n",
      "['unsplit', 'infinitive', 'satisfy', 'grammar', 'checker', 'stephen', 'pave', 'adverb', 'delete']\n",
      "['picture', 'normally', 'worth', 'thousand', 'words', 'picture', 'hotel', 'website', 'somehow', 'convey', 'information']\n",
      "['quiet', 'outside', 'zipper', 'camping', 'memory', 'haiku', 'myfirstone']\n",
      "['weird', 'cigarette', 'butt', 'consider', 'litter', 'people', 'throw', 'wherever']\n",
      "['talking', 'monster', 'joke', 'matter']\n",
      "['movie', 'always', 'scientist']\n",
      "['want', 'grizzly', 'adams', 'except', 'instead', 'suburb']\n",
      "['making', 'mistake', 'stage', 'life']\n",
      "['paper', 'notebook', 'always', 'line', 'rule', 'system']\n",
      "['google', 'really', 'need', 'paste', 'special', 'default', 'paste', 'without', 'formatting']\n",
      "['could', 'fully', 'trust', 'documentary', 'filmmaker', 'expect', 'people', 'spend', 'something', 'report']\n",
      "[\"haven't\", 'twitter', 'whale', 'funny', 'appreciate', 'annoyance']\n",
      "['traveling', 'means', 'expose', 'world', 'going', 'handbasket']\n",
      "['single', 'container', 'pollute', 'million', 'truck', 'right']\n",
      "['garage', 'opener', 'push', 'little', 'button', 'storiesbeforetheinternet']\n",
      "['funny', 'listening', 'something', 'click', 'suddenly', 'brain', 'encode']\n",
      "['first', 'mouse', 'windows', 'weird', 'click', 'active', 'mouse', 'pointer', 'sitting', 'distract']\n",
      "['frighten', 'prefer', 'little', 'alcohol', 'science']\n",
      "['something', 'little', 'trick']\n",
      "['windows', 'three', 'option', 'addition', 'third', 'button']\n",
      "['family', 'watch', 'jumanji', 'pretty', 'movie', 'take', 'place', 'watch']\n",
      "['move', 'cheap', 'laptop', 'cloud', 'suddenly', 'resource', 'constrain', 'paybythebit']\n",
      "['walking', 'fancy', 'neighborhood', 'resident', 'suspiciously', 'want', 'alarm', \"ma'am\", 'computer', 'scientist']\n",
      "['amaze', 'worthless', 'would', '10,000', 'years', 'could', 'describe', 'amaze', 'technology', 'could', 'build']\n",
      "['thought', 'things', '25-year', 'worker', 'asking', 'hear', 'name', 'magic']\n",
      "['outlook', 'unable', 'comprehend', 'email', 'another', 'device', 'keep', 'saying', 'hours', 'meeting']\n",
      "['scientist', 'earth', 'endanger', 'strain', 'resistant', 'humans']\n",
      "['instead', 'viewing', 'customer', 'service', 'company', 'treat', 'investment', 'information', 'gathering']\n",
      "['remember', 'bruce', 'springsteen', 'river', 'union', 'union', 'wages']\n",
      "['depress', 'company', 'block', 'something', 'everyone', 'power']\n",
      "['understand', 'seem', 'unwrap', 'finger', 'directly', 'suppose', 'remain', 'sterile']\n",
      "['people', 'artificial', 'intelligence', 'surprise', 'works', 'surprise', 'gradual', 'notice']\n",
      "['windows', 'frequently', 'crash', 'annoy', 'document', 'recovery', 'silliness', 'everything', 'leave', 'alone']\n",
      "['robot', 'video', 'second', 'bullying', 'message', 'second', 'nightmare', 'vivid', 'enough']\n",
      "['kevin', 'kelly', 'driving', 'inhumanly', 'focus', 'obsess', 'argument', 'garage']\n",
      "['amaze', 'take', 'write']\n",
      "['putting', 'peanut', 'butter', 'stand', 'assembly', 'without']\n",
      "['flight', 'coffee', 'wonder', 'tax']\n",
      "['hunger', 'game', 'mockingjay', 'spoof', 'shot', 'spoof', 'mock']\n",
      "['nothing', 'excite', 'depress', 'figuring', 'cause', 'happy', 'regret', 'waste']\n",
      "['driver', 'decide', 'killing', 'economical', 'careful', 'walking', 'china']\n",
      "['watch', 'space', 'odyssey', 'old', 'pretty', 'angry', 'worst', 'movie']\n",
      "['beautiful', 'innovative', 'video', 'worth', 'watching', 'SCREEN_NAME']\n",
      "['night', 'finally', 'finish', 'watching', 'america', 'encompass', 'event', 'lifetime']\n",
      "['move', 'texas', 'breakfast', 'ask', 'want', 'gravy', 'look', 'stupefy']\n",
      "['semantic', 'necessary', 'seem', 'crumble', 'fracture', 'complexity', 'around', 'anyone', 'replacing']\n",
      "[]\n",
      "['clearing', 'house', 'build', 'subdivision', 'call', 'preserve']\n",
      "['years', 'people', 'playing', 'baseball', 'trail', 'right', 'guess']\n",
      "['hiking', 'shoes', 'apart', 'mile', \"haven't\", 'barefoot', 'since', 'step', 'break', 'glass', 'SCREEN_NAME']\n",
      "['funny', 'someone', 'edit', 'comment', 'beginning', 'trail', 'write', 'better', 'toward']\n",
      "['gradual', 'approach', 'driving', 'either', 'something', 'something']\n",
      "['spend', 'hack', 'undergrowth', 'making', 'progress', 'hopefully', 'clearing', 'tomorrow']\n",
      "['local', 'newspaper', 'report', 'resident', 'want', 'street', 'widen', 'emergency', 'vehicle', 'ask', 'want', 'smaller', 'yard']\n",
      "['apple', 'calorie', 'big']\n",
      "['someone', 'write', 'browser', 'plugin', 'hide', 'stock', 'photograph', 'clutter', 'information', 'would']\n",
      "['people', 'black', 'white', 'picture', 'brain', 'initially', 'conclude', 'traveler', 'remember', 'monkey']\n",
      "['student', 'today', 'fancy', 'internet', 'encyclopedia', 'limited', 'report', 'could']\n",
      "['every', 'thing', 'today', 'problem', 'operand', 'broadcast', 'header', 'file', 'found', 'deploy', 'refuse', 'dance']\n",
      "['watch', 'movie', 'inherent', 'going', 'love', 'movie']\n",
      "['school', 'pizza', 'place', 'manager', 'take', 'pizza', 'look', 'except', 'cheese', 'backtowashingdishes']\n",
      "['estimate', 'young', 'spend', 'waking', 'looking', 'misplace', 'object']\n",
      "['bazooka', 'anymore']\n",
      "['never', 'understand', 'writer', 'block', 'today', 'write', 'realize', 'writer', 'block', 'water']\n",
      "['people', 'hungry', 'suffer', 'suffering', 'better']\n",
      "['recently', 'watch', 'cloud', 'atlas', 'applaud', 'effort', 'making', 'something', 'different']\n",
      "['finally', 'reading', 'runner', 'thought', 'would', 'cheesy', 'happy', 'expect', 'deliverance']\n",
      "['someday', 'going', 'write', 'whole', 'chapter', 'parenthetical']\n",
      "['retro', 'arcade', 'place', 'child', 'party', 'atari', 'play', 'combat', 'chance']\n",
      "['worry', 'coach', 'yell', 'worry', 'coach', 'stops', 'yelling', 'broadlyapplicable']\n",
      "['wonder', 'plan', 'unhappiness', 'without', 'plan']\n",
      "['always', 'wonder', 'would', 'along', 'internet', 'baseball']\n",
      "['bucket', 'popcorn', 'matter', 'movie', 'happy']\n",
      "['could', 'lunch', 'right']\n",
      "['article', 'people', 'want', 'wide', 'street', 'change', 'would', 'inabilitytoreason']\n",
      "['younger', 'playing', 'soccer', 'actually', 'better', 'longer', 'works']\n",
      "['tire', 'productive']\n",
      "['rule', 'seriously', 'silly', 'yell', 'china', 'choose', 'sweet', 'potato', 'second', 'lunch', 'vegetable']\n",
      "['wireless', 'customer', 'service', 'person', 'complain', 'cutting', 'funny']\n",
      "['notice', 'saying', 'license', 'plate', 'understand']\n",
      "['start', 'typing', 'search', 'google', 'come', 'aware', 'roomba', 'anxiety']\n",
      "['polite', 'asking']\n",
      "['invitation', 'submit', 'paper', 'conference', 'science', 'information', 'seem', 'broad']\n",
      "['getting', 'buying', 'instead', 'excite', 'event']\n",
      "['seem', 'monkey', 'climb', 'standing', 'right']\n",
      "['chocolate', 'cookie', 'dough', 'cream', 'health', 'productivity', 'least', 'minutes']\n",
      "['madness', 'come', 'tomorrow']\n",
      "['dream', 'playing', 'little', 'league', 'baseball', 'drove', 'garbage', 'truck']\n",
      "['comma', 'splice', 'scientific', 'proposal', 'reject']\n",
      "['thinking', 'writing', 'avoid', 'sentence', 'distract', 'garden', 'interpretation', 'boat']\n",
      "['rhythm', 'vowel']\n",
      "['imaginary', 'friend', 'anymore', 'require', 'solitude', 'longer', 'exist', 'wire', 'society']\n",
      "['young', 'people', 'sometimes', 'book', 'watch', 'movie', 'multiple', 'times', 'older']\n",
      "['warning', 'danger', 'heart', 'disease', 'accident', 'cancer', 'nobody', 'talks', 'stairs']\n",
      "['trail', 'amaze', 'stuff', 'consume', '14,000', 'calorie', 'still', 'hungry']\n",
      "['given', 'important', 'hiding', 'seeking', 'ancestor', 'think', 'would', 'better', 'giggle', 'peek']\n",
      "['terrify', 'werewolf', 'history', 'channel', 'spend', 'try', 'explain', 'viewers']\n",
      "['marshmallow', 'knowing', 'everyone', 'always', 'vaguely', 'uncomfortable']\n",
      "['finally', 'ending']\n",
      "['write', 'later', 'remember', 'refer']\n",
      "['tricky', 'right', 'level', 'office', 'gossip', 'engage', 'contrary', 'repeat', 'advice', 'snooty']\n",
      "['rarely', 'phone', 'call', 'realize', 'complicate', 'depend', 'receiver']\n",
      "['picking', 'movie', 'learn', 'enjoy', 'hours', 'enjoy']\n",
      "['admit', 'wordless', 'wednesday', 'means']\n",
      "['ask', 'nonfiction', 'bigfoot', 'library']\n",
      "['today', 'weird', 'place', 'picture', 'pretty', 'people', 'everywhere', 'familiar']\n",
      "['theory', 'come', 'forget', 'similar', 'memory', 'trigger', 'current', 'moment']\n",
      "['fix', 'leaky', 'faucet', 'today', 'three', 'guess', 'adult', 'feel']\n",
      "['peeve', 'author', 'write', 'people', 'finally', 'realize']\n",
      "['weird', 'music', 'make', 'everything', 'excite', 'movie', 'video', 'party', 'social', 'brain']\n",
      "['wonder', 'raise', 'movie', 'clear', 'world', 'people']\n",
      "['unfair', 'young', 'candy', 'older', 'afford', 'candy']\n",
      "['studio', 'individual', 'scene', 'linkable', 'funny', 'point', 'perfect', 'advertising']\n",
      "['drove', 'would', 'siren', 'police', 'engine']\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "import random\n",
    "import os\n",
    "\n",
    "text_data = []\n",
    "filepath = os.path.join('..','resources', 'jonathan_mugan_tweets.txt')\n",
    "\n",
    "with open(filepath) as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .95:\n",
    "            print(tokens)\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary fromthe data\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to a bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the corpus and dictionary, we will use these in another video to visualize\n",
    "import pickle\n",
    "pickle.dump(corpus, open(\"corpus.pkl\", \"wb\"))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS,\n",
    "                                           id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.012*\"would\" + 0.011*\"robot\" + 0.009*\"could\" + 0.007*\"try\"'), (1, '0.017*\"funny\" + 0.013*\"remember\" + 0.012*\"memory\" + 0.008*\"seem\"'), (2, '0.013*\"movie\" + 0.009*\"world\" + 0.007*\"computer\" + 0.007*\"great\"'), (3, '0.010*\"always\" + 0.009*\"funny\" + 0.009*\"think\" + 0.009*\"coffee\"'), (4, '0.015*\"watch\" + 0.014*\"people\" + 0.011*\"would\" + 0.009*\"dream\"')]\n",
      "[(0, '0.012*\"would\" + 0.011*\"robot\" + 0.009*\"could\" + 0.007*\"try\"'), (1, '0.017*\"funny\" + 0.013*\"remember\" + 0.012*\"memory\" + 0.008*\"seem\"'), (2, '0.013*\"movie\" + 0.009*\"world\" + 0.007*\"computer\" + 0.007*\"great\"'), (3, '0.010*\"always\" + 0.009*\"funny\" + 0.009*\"think\" + 0.009*\"coffee\"'), (4, '0.015*\"watch\" + 0.014*\"people\" + 0.011*\"would\" + 0.009*\"dream\"')]\n",
      "[(0, '0.012*\"would\" + 0.011*\"robot\" + 0.009*\"could\" + 0.007*\"try\"'), (1, '0.017*\"funny\" + 0.013*\"remember\" + 0.012*\"memory\" + 0.008*\"seem\"'), (2, '0.013*\"movie\" + 0.009*\"world\" + 0.007*\"computer\" + 0.007*\"great\"'), (3, '0.010*\"always\" + 0.009*\"funny\" + 0.009*\"think\" + 0.009*\"coffee\"'), (4, '0.015*\"watch\" + 0.014*\"people\" + 0.011*\"would\" + 0.009*\"dream\"')]\n",
      "[(0, '0.012*\"would\" + 0.011*\"robot\" + 0.009*\"could\" + 0.007*\"try\"'), (1, '0.017*\"funny\" + 0.013*\"remember\" + 0.012*\"memory\" + 0.008*\"seem\"'), (2, '0.013*\"movie\" + 0.009*\"world\" + 0.007*\"computer\" + 0.007*\"great\"'), (3, '0.010*\"always\" + 0.009*\"funny\" + 0.009*\"think\" + 0.009*\"coffee\"'), (4, '0.015*\"watch\" + 0.014*\"people\" + 0.011*\"would\" + 0.009*\"dream\"')]\n",
      "[(0, '0.012*\"would\" + 0.011*\"robot\" + 0.009*\"could\" + 0.007*\"try\"'), (1, '0.017*\"funny\" + 0.013*\"remember\" + 0.012*\"memory\" + 0.008*\"seem\"'), (2, '0.013*\"movie\" + 0.009*\"world\" + 0.007*\"computer\" + 0.007*\"great\"'), (3, '0.010*\"always\" + 0.009*\"funny\" + 0.009*\"think\" + 0.009*\"coffee\"'), (4, '0.015*\"watch\" + 0.014*\"people\" + 0.011*\"would\" + 0.009*\"dream\"')]\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1), (193, 1)]\n",
      "[(0, 0.066820683571410217), (1, 0.066876080239630362), (2, 0.068971485015669895), (3, 0.066667028480243237), (4, 0.73066472269304616)]\n"
     ]
    }
   ],
   "source": [
    "# try a new document\n",
    "# we it is mostly topic 3\n",
    "new_doc = 'I watch movies.'\n",
    "new_doc = prepare_text_for_lda(new_doc)\n",
    "new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "print(new_doc_bow)\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.011*\"funny\" + 0.011*\"people\" + 0.010*\"watch\" + 0.009*\"always\"'), (1, '0.005*\"could\" + 0.005*\"really\" + 0.005*\"going\" + 0.005*\"things\"'), (2, '0.016*\"would\" + 0.008*\"coffee\" + 0.008*\"people\" + 0.007*\"computer\"')]\n",
      "[(0, '0.011*\"funny\" + 0.011*\"people\" + 0.010*\"watch\" + 0.009*\"always\"'), (1, '0.005*\"could\" + 0.005*\"really\" + 0.005*\"going\" + 0.005*\"things\"'), (2, '0.016*\"would\" + 0.008*\"coffee\" + 0.008*\"people\" + 0.007*\"computer\"')]\n",
      "[(0, '0.011*\"funny\" + 0.011*\"people\" + 0.010*\"watch\" + 0.009*\"always\"'), (1, '0.005*\"could\" + 0.005*\"really\" + 0.005*\"going\" + 0.005*\"things\"'), (2, '0.016*\"would\" + 0.008*\"coffee\" + 0.008*\"people\" + 0.007*\"computer\"')]\n"
     ]
    }
   ],
   "source": [
    "# try three topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3,\n",
    "                                           id2word=dictionary, passes=15)\n",
    "ldamodel.save('model3.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.028*\"remember\" + 0.023*\"dream\" + 0.016*\"still\" + 0.016*\"memory\"'), (1, '0.025*\"would\" + 0.021*\"people\" + 0.013*\"wrong\" + 0.010*\"world\"'), (2, '0.011*\"hungry\" + 0.010*\"try\" + 0.009*\"people\" + 0.009*\"years\"'), (3, '0.014*\"always\" + 0.013*\"could\" + 0.012*\"notice\" + 0.010*\"feel\"'), (4, '0.018*\"watch\" + 0.013*\"coffee\" + 0.011*\"understand\" + 0.011*\"recently\"'), (5, '0.016*\"coffee\" + 0.015*\"article\" + 0.014*\"write\" + 0.012*\"drink\"'), (6, '0.014*\"child\" + 0.014*\"seem\" + 0.011*\"story\" + 0.011*\"people\"'), (7, '0.022*\"could\" + 0.021*\"going\" + 0.018*\"robot\" + 0.013*\"want\"'), (8, '0.020*\"people\" + 0.012*\"make\" + 0.012*\"first\" + 0.011*\"google\"'), (9, '0.023*\"movie\" + 0.014*\"machine\" + 0.012*\"computer\" + 0.011*\"start\"')]\n",
      "[(0, '0.028*\"remember\" + 0.023*\"dream\" + 0.016*\"still\" + 0.016*\"memory\"'), (1, '0.025*\"would\" + 0.021*\"people\" + 0.013*\"wrong\" + 0.010*\"world\"'), (2, '0.011*\"hungry\" + 0.010*\"try\" + 0.009*\"people\" + 0.009*\"years\"'), (3, '0.014*\"always\" + 0.013*\"could\" + 0.012*\"notice\" + 0.010*\"feel\"'), (4, '0.018*\"watch\" + 0.013*\"coffee\" + 0.011*\"understand\" + 0.011*\"recently\"'), (5, '0.016*\"coffee\" + 0.015*\"article\" + 0.014*\"write\" + 0.012*\"drink\"'), (6, '0.014*\"child\" + 0.014*\"seem\" + 0.011*\"story\" + 0.011*\"people\"'), (7, '0.022*\"could\" + 0.021*\"going\" + 0.018*\"robot\" + 0.013*\"want\"'), (8, '0.020*\"people\" + 0.012*\"make\" + 0.012*\"first\" + 0.011*\"google\"'), (9, '0.023*\"movie\" + 0.014*\"machine\" + 0.012*\"computer\" + 0.011*\"start\"')]\n",
      "[(0, '0.028*\"remember\" + 0.023*\"dream\" + 0.016*\"still\" + 0.016*\"memory\"'), (1, '0.025*\"would\" + 0.021*\"people\" + 0.013*\"wrong\" + 0.010*\"world\"'), (2, '0.011*\"hungry\" + 0.010*\"try\" + 0.009*\"people\" + 0.009*\"years\"'), (3, '0.014*\"always\" + 0.013*\"could\" + 0.012*\"notice\" + 0.010*\"feel\"'), (4, '0.018*\"watch\" + 0.013*\"coffee\" + 0.011*\"understand\" + 0.011*\"recently\"'), (5, '0.016*\"coffee\" + 0.015*\"article\" + 0.014*\"write\" + 0.012*\"drink\"'), (6, '0.014*\"child\" + 0.014*\"seem\" + 0.011*\"story\" + 0.011*\"people\"'), (7, '0.022*\"could\" + 0.021*\"going\" + 0.018*\"robot\" + 0.013*\"want\"'), (8, '0.020*\"people\" + 0.012*\"make\" + 0.012*\"first\" + 0.011*\"google\"'), (9, '0.023*\"movie\" + 0.014*\"machine\" + 0.012*\"computer\" + 0.011*\"start\"')]\n",
      "[(0, '0.028*\"remember\" + 0.023*\"dream\" + 0.016*\"still\" + 0.016*\"memory\"'), (1, '0.025*\"would\" + 0.021*\"people\" + 0.013*\"wrong\" + 0.010*\"world\"'), (2, '0.011*\"hungry\" + 0.010*\"try\" + 0.009*\"people\" + 0.009*\"years\"'), (3, '0.014*\"always\" + 0.013*\"could\" + 0.012*\"notice\" + 0.010*\"feel\"'), (4, '0.018*\"watch\" + 0.013*\"coffee\" + 0.011*\"understand\" + 0.011*\"recently\"'), (5, '0.016*\"coffee\" + 0.015*\"article\" + 0.014*\"write\" + 0.012*\"drink\"'), (6, '0.014*\"child\" + 0.014*\"seem\" + 0.011*\"story\" + 0.011*\"people\"'), (7, '0.022*\"could\" + 0.021*\"going\" + 0.018*\"robot\" + 0.013*\"want\"'), (8, '0.020*\"people\" + 0.012*\"make\" + 0.012*\"first\" + 0.011*\"google\"'), (9, '0.023*\"movie\" + 0.014*\"machine\" + 0.012*\"computer\" + 0.011*\"start\"')]\n",
      "[(0, '0.028*\"remember\" + 0.023*\"dream\" + 0.016*\"still\" + 0.016*\"memory\"'), (1, '0.025*\"would\" + 0.021*\"people\" + 0.013*\"wrong\" + 0.010*\"world\"'), (2, '0.011*\"hungry\" + 0.010*\"try\" + 0.009*\"people\" + 0.009*\"years\"'), (3, '0.014*\"always\" + 0.013*\"could\" + 0.012*\"notice\" + 0.010*\"feel\"'), (4, '0.018*\"watch\" + 0.013*\"coffee\" + 0.011*\"understand\" + 0.011*\"recently\"'), (5, '0.016*\"coffee\" + 0.015*\"article\" + 0.014*\"write\" + 0.012*\"drink\"'), (6, '0.014*\"child\" + 0.014*\"seem\" + 0.011*\"story\" + 0.011*\"people\"'), (7, '0.022*\"could\" + 0.021*\"going\" + 0.018*\"robot\" + 0.013*\"want\"'), (8, '0.020*\"people\" + 0.012*\"make\" + 0.012*\"first\" + 0.011*\"google\"'), (9, '0.023*\"movie\" + 0.014*\"machine\" + 0.012*\"computer\" + 0.011*\"start\"')]\n",
      "[(0, '0.028*\"remember\" + 0.023*\"dream\" + 0.016*\"still\" + 0.016*\"memory\"'), (1, '0.025*\"would\" + 0.021*\"people\" + 0.013*\"wrong\" + 0.010*\"world\"'), (2, '0.011*\"hungry\" + 0.010*\"try\" + 0.009*\"people\" + 0.009*\"years\"'), (3, '0.014*\"always\" + 0.013*\"could\" + 0.012*\"notice\" + 0.010*\"feel\"'), (4, '0.018*\"watch\" + 0.013*\"coffee\" + 0.011*\"understand\" + 0.011*\"recently\"'), (5, '0.016*\"coffee\" + 0.015*\"article\" + 0.014*\"write\" + 0.012*\"drink\"'), (6, '0.014*\"child\" + 0.014*\"seem\" + 0.011*\"story\" + 0.011*\"people\"'), (7, '0.022*\"could\" + 0.021*\"going\" + 0.018*\"robot\" + 0.013*\"want\"'), (8, '0.020*\"people\" + 0.012*\"make\" + 0.012*\"first\" + 0.011*\"google\"'), (9, '0.023*\"movie\" + 0.014*\"machine\" + 0.012*\"computer\" + 0.011*\"start\"')]\n",
      "[(0, '0.028*\"remember\" + 0.023*\"dream\" + 0.016*\"still\" + 0.016*\"memory\"'), (1, '0.025*\"would\" + 0.021*\"people\" + 0.013*\"wrong\" + 0.010*\"world\"'), (2, '0.011*\"hungry\" + 0.010*\"try\" + 0.009*\"people\" + 0.009*\"years\"'), (3, '0.014*\"always\" + 0.013*\"could\" + 0.012*\"notice\" + 0.010*\"feel\"'), (4, '0.018*\"watch\" + 0.013*\"coffee\" + 0.011*\"understand\" + 0.011*\"recently\"'), (5, '0.016*\"coffee\" + 0.015*\"article\" + 0.014*\"write\" + 0.012*\"drink\"'), (6, '0.014*\"child\" + 0.014*\"seem\" + 0.011*\"story\" + 0.011*\"people\"'), (7, '0.022*\"could\" + 0.021*\"going\" + 0.018*\"robot\" + 0.013*\"want\"'), (8, '0.020*\"people\" + 0.012*\"make\" + 0.012*\"first\" + 0.011*\"google\"'), (9, '0.023*\"movie\" + 0.014*\"machine\" + 0.012*\"computer\" + 0.011*\"start\"')]\n",
      "[(0, '0.028*\"remember\" + 0.023*\"dream\" + 0.016*\"still\" + 0.016*\"memory\"'), (1, '0.025*\"would\" + 0.021*\"people\" + 0.013*\"wrong\" + 0.010*\"world\"'), (2, '0.011*\"hungry\" + 0.010*\"try\" + 0.009*\"people\" + 0.009*\"years\"'), (3, '0.014*\"always\" + 0.013*\"could\" + 0.012*\"notice\" + 0.010*\"feel\"'), (4, '0.018*\"watch\" + 0.013*\"coffee\" + 0.011*\"understand\" + 0.011*\"recently\"'), (5, '0.016*\"coffee\" + 0.015*\"article\" + 0.014*\"write\" + 0.012*\"drink\"'), (6, '0.014*\"child\" + 0.014*\"seem\" + 0.011*\"story\" + 0.011*\"people\"'), (7, '0.022*\"could\" + 0.021*\"going\" + 0.018*\"robot\" + 0.013*\"want\"'), (8, '0.020*\"people\" + 0.012*\"make\" + 0.012*\"first\" + 0.011*\"google\"'), (9, '0.023*\"movie\" + 0.014*\"machine\" + 0.012*\"computer\" + 0.011*\"start\"')]\n",
      "[(0, '0.028*\"remember\" + 0.023*\"dream\" + 0.016*\"still\" + 0.016*\"memory\"'), (1, '0.025*\"would\" + 0.021*\"people\" + 0.013*\"wrong\" + 0.010*\"world\"'), (2, '0.011*\"hungry\" + 0.010*\"try\" + 0.009*\"people\" + 0.009*\"years\"'), (3, '0.014*\"always\" + 0.013*\"could\" + 0.012*\"notice\" + 0.010*\"feel\"'), (4, '0.018*\"watch\" + 0.013*\"coffee\" + 0.011*\"understand\" + 0.011*\"recently\"'), (5, '0.016*\"coffee\" + 0.015*\"article\" + 0.014*\"write\" + 0.012*\"drink\"'), (6, '0.014*\"child\" + 0.014*\"seem\" + 0.011*\"story\" + 0.011*\"people\"'), (7, '0.022*\"could\" + 0.021*\"going\" + 0.018*\"robot\" + 0.013*\"want\"'), (8, '0.020*\"people\" + 0.012*\"make\" + 0.012*\"first\" + 0.011*\"google\"'), (9, '0.023*\"movie\" + 0.014*\"machine\" + 0.012*\"computer\" + 0.011*\"start\"')]\n",
      "[(0, '0.028*\"remember\" + 0.023*\"dream\" + 0.016*\"still\" + 0.016*\"memory\"'), (1, '0.025*\"would\" + 0.021*\"people\" + 0.013*\"wrong\" + 0.010*\"world\"'), (2, '0.011*\"hungry\" + 0.010*\"try\" + 0.009*\"people\" + 0.009*\"years\"'), (3, '0.014*\"always\" + 0.013*\"could\" + 0.012*\"notice\" + 0.010*\"feel\"'), (4, '0.018*\"watch\" + 0.013*\"coffee\" + 0.011*\"understand\" + 0.011*\"recently\"'), (5, '0.016*\"coffee\" + 0.015*\"article\" + 0.014*\"write\" + 0.012*\"drink\"'), (6, '0.014*\"child\" + 0.014*\"seem\" + 0.011*\"story\" + 0.011*\"people\"'), (7, '0.022*\"could\" + 0.021*\"going\" + 0.018*\"robot\" + 0.013*\"want\"'), (8, '0.020*\"people\" + 0.012*\"make\" + 0.012*\"first\" + 0.011*\"google\"'), (9, '0.023*\"movie\" + 0.014*\"machine\" + 0.012*\"computer\" + 0.011*\"start\"')]\n"
     ]
    }
   ],
   "source": [
    "# try ten topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10,\n",
    "                                           id2word=dictionary, passes=15)\n",
    "ldamodel.save('model10.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
